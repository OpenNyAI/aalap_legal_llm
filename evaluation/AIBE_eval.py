import json
import openai
import re
from tqdm import tqdm
import time
from text_generation import Client
from pqdm.processes import pqdm
from utils import call_openai_api
class AIBE_eval:
    def __init__(self, ip_address='localhost', port='8080'):
        self.aibe_data_path = '/Users/prathamesh/tw_projects/OpenNyAI/data/jugalbandi/AIBE/aibe_chatgpt_answers_full.json'
        self.aibe_qa = json.load(open(self.aibe_data_path))
        endpoint_url = 'http://'+ip_address+ ':' + port +'/'
        self.llm_client = Client(endpoint_url, timeout=1000)
        self.llm_name = 'aalap_full'
        self.system_prompt = 'Answer the following question in context of Indian Legal Systems. Choose one of the 4 options "A","B","C" or "D".'

    def _get_chatgpt_answers(self,question_dict,model_name):
        question = question_dict['question_text']
        for option,option_text in question_dict['options'].items():
            question = question + '\n' + option + ": "+ option_text

        messages = [{"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": question}]
        completions = openai.ChatCompletion.create(model=model_name, messages= messages,
                                               max_tokens=100, n=1, stop=None, temperature=0)
        ai_answer = completions.choices[0].message.content
        ai_answer = re.sub(r'^([ABCD])\:.*',r'\1',ai_answer.strip())
        question_dict['ai_answer'] = ai_answer

    def _chatgpt_eval(self,question_dict):
        if question_dict.get('chatgpt_answer') is None:
            try:
                self._get_chatgpt_answers(question_dict, 'gpt-3.5-turbo')
                time.sleep(1)
            except:
                print('could not get answer from ChatGPT for ' + str(question_dict['q_no']))
                time.sleep(5)

        if question_dict.get('chatgpt_answer'):
            if question_dict['chatgpt_answer'] == question_dict['answer']:
                question_dict['chatgpt_answer_category'] = 'Correct'
            else:
                question_dict['chatgpt_answer_category'] = 'Incorrect'

    def call_llm_api(self, system_prompt, user_prompt, max_new_tokens):
        prompt = f"<s>[INST] <<SYS>>{system_prompt}<</SYS>>\n\n{user_prompt}\n[/INST]"
        # prompt = user_prompt
        generated_text = self.llm_client.generate(prompt,
                                                  max_new_tokens=max_new_tokens,
                                                  repetition_penalty=1.03,
                                                  return_full_text=False,
                                                  stop_sequences=["</s>"],
                                                  temperature=0.1,
                                                  top_k=10,
                                                  top_n_tokens=5,
                                                  top_p=0.95,
                                                  typical_p=0.95).generated_text

        generated_text = re.sub('^\s+Response:\s+','',generated_text)
        return generated_text.strip()

    def _compare_ground_truth_with_answer_regex(self,generated_text,ground_truth):
        ## searches for ground truth answer in generated text. Generated text might contain more words as well. Hence if the ground truth is present in generated text then we treat as a match.
        if re.search(r'\b'+ground_truth+r'\b',generated_text):
            return True
        else:
            return False

    def _chatgpt_assement(self,task_description,llm_response,ground_truth):

        #task_description = task_description.replace(self.explain_answer_prompt,'')
        eval_prompt = f'''Task:{task_description}\n\nLLM Response: {llm_response}\n\nGround Truth: {ground_truth}'''
        messages = [
            {"role": "system", "content": 'Your job to to evaluate response generated by LLM against the ground truth answer provided below. Only output one word "correct" or "incorrect". Do not generate any other text.'},
            {"role": "user", "content": eval_prompt}
        ]
        chatgpt_assesment = call_openai_api(messages, max_tokens=20, model='gpt-3.5-turbo')
        if chatgpt_assesment == 'correct':
            return True
        elif chatgpt_assesment == 'incorrect':
            return False
        else:
            return None
    def _llm_eval(self, question_dict):
        if question_dict.get(self.llm_name+'_answer') is None:
            try:
                question_dict[self.llm_name + '_answer'] = self.call_llm_api(self.system_prompt,question_dict['question_text'],max_new_tokens=1024)

            except:
                print('could not get answer from LLM for ' + str(question_dict['q_no']))
                time.sleep(5)

        if question_dict.get(self.llm_name+'_answer'):
            task_description = question_dict['question_text']
            for option, option_text in question_dict['options'].items():
                task_description = task_description + '\n' + option + ": " + option_text
            question_dict[self.llm_name+'_answer_correct'] = self._chatgpt_assement(task_description,question_dict[self.llm_name+'_answer'],question_dict['answer'])

            # if self._compare_ground_truth_with_answer_regex(question_dict[self.llm_name+'_answer'],question_dict['answer']):
            #     question_dict[self.llm_name+'_answer_category'] = 'Correct'
            # else:
            #     question_dict[self.llm_name+'_answer_category'] = 'Incorrect'

    def _evaluate_one_question_all_models(self,question_dict):
        #self._chatgpt_eval(question_dict)
        self._llm_eval(question_dict)
        return question_dict
    def evaluate(self):
        eval_results = pqdm(self.aibe_qa, self._evaluate_one_question_all_models, n_jobs=5)
        json.dump(eval_results, open(self.aibe_data_path, 'w'), indent=4)


if __name__ == '__main__':
    a = AIBE_eval()
    a.evaluate()

